{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Motivación:\n",
    "\n",
    "Machine Learning \"No free-lunch theorem\":\n",
    "\n",
    "- Distintos algoritmos funcionan correctamente en distintas situaciones\n",
    "\n",
    "- A priori no es factible saber qué algoritmo funcionará mejor\n",
    "\n",
    "- Por más que ajuste muy bien en el conjunto de test, aún puede estar fallando en algunos casos y podría existir otro algoritmo que a esos los ajuste mejor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A la hora de predecir, combinar distintas opiniones puede ser una excelente idea (como cuando vamos al médico) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/Ensembles_example.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Formas de caracterización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Utilizando distintos subconjuntos de variables de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/Ensembles_type_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Utilizando el mismo conjunto de variables de entrada, pero distintos algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/Ensembles_type_2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependiendo de la forma de los clasificadores base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Homogéneos: conjunto de modelos de un mismo tipo (Ejemplo: todas redes neuronales).\n",
    "- Heterogéneos: conjunto de modelos de distintos tipos (Ejemplo: redes neuronales, k-NN, logistic regression, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependiendo la estructura final del ensemble (fase de predicción)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Paralelos: Todos los modelos base realizan una predicción y se combinan de alguna manera para obtener una única salida.\n",
    "- En serie: Se consultan los modelos base de forma secuencial, donde cada uno recibe los datos de entrada y la salida del modelo previo.\n",
    "- Jerárquicos: Se establece una jerarquía y las salidas de los modelos base constituyen las entradas de un metamodelo padre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resumen parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Tenemos modelos base que sirven para obtener distintas opiniones sobre cada caso.\n",
    "- Cada uno de ellos se ajusta mejor a una serie de casos y peor en otros.\n",
    "- Lo ideal es combinar modelos sencillos.\n",
    "- __DEBE EXISTIR VARIABILIDAD EN LAS COMBINACIONES !__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Métodos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Fusión de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:56:23.831978Z",
     "start_time": "2021-04-01T20:56:23.821927Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 5 clasificadores, 3 etiquetas posibles (0, 1 o 2).\n",
    "outputs = [[0,1,0], [1,0,0], [1,0,0], [0,1,0], [0,1,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Voto por mayoría:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:56:30.023136Z",
     "start_time": "2021-04-01T20:56:29.865074Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados parciales: [2 3 0]\n",
      "Etiqueta final: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Resultados parciales:', np.sum(outputs, axis=0))\n",
    "print('Etiqueta final:', np.sum(outputs, axis=0).argmax(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Voto por mayoría ponderado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T02:49:58.457896Z",
     "start_time": "2020-04-27T02:49:58.450246Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas ponderadas: [array([0, 2, 0]), array([1, 0, 0]), array([3, 0, 0]), array([0, 1, 0]), array([0, 3, 0])]\n",
      "Resultados parciales: [4 6 0]\n",
      "Etiqueta final: 1\n"
     ]
    }
   ],
   "source": [
    "classifiers_weigth = [2,1,3,1,3]\n",
    "partial_sum = [cw * np.array(o) for cw, o in zip(classifiers_weigth, outputs)]\n",
    "print('Etiquetas ponderadas:', partial_sum)\n",
    "print('Resultados parciales:', np.sum(partial_sum, axis=0))\n",
    "print('Etiqueta final:', np.sum(partial_sum, axis=0).argmax(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Usando probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:57:28.339625Z",
     "start_time": "2021-04-01T20:57:28.334862Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 5 estimadores, 3 probabilidades para las etiquetas posibles (0, 1 o 2).\n",
    "outputs = [[0.1,0.5,0.4], [0,0,1.0], [0.4,0.3,0.3], [0.2,0.7,0.1], [0.1,0.8,0.1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Media aritmética"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:57:28.955837Z",
     "start_time": "2021-04-01T20:57:28.951034Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_0 = 0.16\n",
      "P_1 = 0.46\n",
      "P_2 = 0.38\n"
     ]
    }
   ],
   "source": [
    "for l in range(3):\n",
    "    print('P_{} = {}'.format(l, round(np.mean([p[l] for p in outputs]), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T20:57:29.781903Z",
     "start_time": "2021-04-01T20:57:29.776911Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4 0.8 1. ]\n"
     ]
    }
   ],
   "source": [
    "# Valor máximo para cada clase\n",
    "print(np.max(outputs, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Métodos avanzados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Bagging: Bootstrap AGGregatING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es una metodología o forma de construir el ensemble (no un algoritmo en sí).\n",
    "- Se crean L conjuntos de datos a partir del conjunto inicial, para entrenar L modelos.\n",
    "- Para predecir se combinan las salidas de los L modelos utilizando alguna técnica (generalmente voto por mayoría).\n",
    "- Cada conjunto de datos se genera utilizando la técnica \"boostrap\" (muestreo con reemplazamiento): algunas instancias van a quedar repetidas y otras se eliminarán.\n",
    "- En clasificación tiene sentido con modelos base que sean inestables ! (pequeños cambios en los datos producen resultados diferentes).\n",
    "- Si tengo muchos datos los conjuntos probablemente sean similares y el método pierde eficacia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T16:12:07.080107Z",
     "start_time": "2020-04-26T16:12:06.959308Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/bootrap_concept.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Procedimiento (clasificación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fase de entrenamiento:\n",
    "\n",
    "1) Para k clasificadores base, generar k conjuntos de datos utilizando la técnica **boostrap** a partir del conjunto de datos original.\n",
    "\n",
    "2) Entrenar k clasificadores, cada uno con un conjunto de los creados en el paso previo.\n",
    "\n",
    "Fase de predicción (por cada muestra o ejemplo):\n",
    "\n",
    "1) Clasificar con los k clasificadores.\n",
    "\n",
    "2) Combinar las salidas y devolver un único resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/ensembles.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es un algoritmo que utiliza Bagging\n",
    "- Ensemble homogéneo y paralelo: árboles como algoritmos base y creados de forma independiente\n",
    "- Introduce aleatoriedad extra: cada nodo del árbol se genera seleccionado un subconjunto aleatorio dentro de los atributos disponibles en cada momento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algunos hyper-parámetros para definir:\n",
    "\n",
    "- Cantidad de árboles\n",
    "- Porcentaje de variables a utilizar en cada split\n",
    "- Y todos los que teníamos para un árbol (profundidad máxima, cantidad de instancias mínimas en cada nodo hoja, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Algunas variantes ...\n",
    "\n",
    "- Pasting: cuando los conjuntos de datos se generan sin reemplazamiento y una cantidad de datos menor a la original.\n",
    "- Random Subspaces: cuando los conjuntos de datos se generan sobre un subconjunto de variables de entrada.\n",
    "- Random Patches: cuando los conjuntos de datos se generan con las dos condiciones previas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Al igual que Bagging, es una metodología o forma de construir el ensemble (no un algoritmo en sí).\n",
    "- Los modelos base se construyen uno después del otro de forma incremental.\n",
    "- La idea subyacente es que cada modelo se concentre más en las instancias o casos donde el anterior falló.\n",
    "- Se arranca entrenando un estimador base con una muestra boostrap, pero a partir de la segunda iteración la probabilidad de seleccionar a cada instancia está condicionada al error que obtuvo el modelo previo (mayor error, mayor probabilidad de ser seleccionado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AdaBoost (ADApting BOOSTing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es un algoritmo que utiliza Boosting.\n",
    "- Ensemble homogéneo y en paralelo: no está condicionado a un tipo en particular, pero el ensemble se compone de estimadores de un mismo tipo, creados de a uno.\n",
    "- Los estimadores base deben ser lo más simple posibles (no tiene sentido poner uno muy bueno de entrada). *The Strength of Weak Learnability*. **Schapire, R.** (1990)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Procedimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fase de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T11:58:59.868799Z",
     "start_time": "2022-04-04T11:58:59.858231Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def AdaBoost_generation(train_data, n_iters):\n",
    "    n = train_data.shape[0]\n",
    "    W = [1 / n] * n\n",
    "    \n",
    "    models = []\n",
    "\n",
    "    for k in range(n_iters):\n",
    "        # Generar muestra utilizando W como distribución de los datos\n",
    "        S = get_data_sample(train_data, W)\n",
    "        # Obtener modelo y obtener predicciones\n",
    "        Cl = train_model(S)\n",
    "        Cl_pred = Cl.pred(S)\n",
    "        # Calcular el error\n",
    "        Cl_e = sum([W[i] for i, c in enumerate(Cl_pred) if c != S[i].label])\n",
    "        # Verificamos que el error no sea extremo (ni muy grande ni tampoco cero).\n",
    "        if Cl_e > 0 and Cl_e < 0.5:\n",
    "            # Obtener factor de corrección de pesos normalizado\n",
    "            w_factor = Cl_e / (1 - Cl_e)\n",
    "\n",
    "            # Se disminuye el peso de la instancia si se clasificó correctamente\n",
    "            W = [(w if Cl_pred[i] != S[i].label else w * w_factor) for i, w in enumerate(W)]\n",
    "            W = W / np.max(W)\n",
    "            \n",
    "            models.append((Cl, w_factor))\n",
    "            \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T14:07:50.964361Z",
     "start_time": "2021-04-03T14:07:50.945130Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16668056, 0.16668056, 0.16668056, 0.07142262, 0.07142262,\n",
       "       0.07142262, 0.07142262, 0.07142262, 0.07142262, 0.07142262])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iteración 1\n",
    "n = 10\n",
    "W = [0.1] * n # [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]  \n",
    "Cl_e = 0.3 # el modelo falló al predecir las 3 primeras instancias\n",
    "w_factor = Cl_e / (1 - Cl_e) # = 0.4285\n",
    "W = [0.1, 0.1, 0.1, 0.04285, 0.04285, 0.04285, 0.04285, 0.04285, 0.04285, 0.04285]\n",
    "W = W / np.sum(W)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Fase de predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def AdaBoost_prediction(sample, models):\n",
    "    # Inicializa un vector de pesos para las clases existentes\n",
    "    c_weights = [0] * len(set(test_data.label))\n",
    "\n",
    "    for (m, w_factor) in models:\n",
    "        # Obtiene como predicción la clase\n",
    "        pred_class = m.predict(sample)\n",
    "        # Suma el factor de corrección del modelo al vector de pesos\n",
    "        # de la clase. > error, < peso\n",
    "        c_weights[pred_class] += np.log(1/w_factor)\n",
    "    \n",
    "    return np.argmax(c_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Es otra variante que utiliza Boosting.\n",
    "- Al igual que los anteriores también es homogéneo, pero a la hora de predecir los modelos base se consultan en serie.\n",
    "- Se basa en ir ajustando estimadores base utilizando las salidas del modelo previo como variable a predecir.\n",
    "- Generalmente se utilizan árboles como estimadores base.\n",
    "- Es uno de los modelos más utilizados (suele dar muy buenos resultados sin tener que preprocesar demasiado los datos).\n",
    "- **xgboost** es una de las librerías más utilizadas: tiene interfaces para varios lenguajes, está súper completa y optimizada !."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ejemplo:\n",
    "\n",
    "a) Vamos a entrenar un modelo A para que prediga cuál va a ser la temperatura de mañana en base a la temperatura de hoy, el viento y la humedad. Y = temperatura\n",
    "\n",
    "b) Cuando conocemos los errores del modelo A a partir de una muestra, ahora entrenamos otro modelo B con los mismos datos de entrada pero como variable a predecir usamos el error del modelo A. Y = temperatura - prediccion_A\n",
    "\n",
    "Una vez que entrenamos a ambos, la salida del método sería predecir con ambos modelos y sumar los resultados.\n",
    "\n",
    "El algoritmo termina cuando el error es 0 o cae por debajo de un umbral definido, o se cumple un número máximo de iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Intuición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T14:23:39.125416Z",
     "start_time": "2021-04-03T14:23:38.839254Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/gradient_boosting_1.webp)\n",
    "\n",
    "Extraído de: https://towardsdatascience.com/all-you-need-to-know-about-gradient-boosting-algorithm-part-1-regression-2520a34a502"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/gradient_boosting_2.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/gradient_boosting_3.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/gradient_boosting_4.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bagging vs. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Similitudes:\n",
    "\n",
    "- Combinan estimadores base de un mismo tipo.\n",
    "- Buscamos perder un poco de interpretabilidad en pos de obtener mejores resultados.\n",
    "- Todos los estimadores base se generan a partir del mismo conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Diferencias:\n",
    "- Con Bagging se puede paralelizar la construcción del ensemble, mientras que Boosting no.\n",
    "- Bagging aisla cada modelo construido, mientras que en Boosting el conocimiento se \"comparte\".\n",
    "- Boosting otorga pesos a los estimadores base mientras que en Bagging todos cuentan por igual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Resumen respecto a la fase de generación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](files/images/ensembles_comp.PNG)\n",
    "<br>\n",
    "<div style=\"text-align: right\">ref: https://quantdare.com/</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
