{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Espacio de discusi√≥n\n",
    "\n",
    "#### ¬øC√≥mo resolver√≠an el problema del Pong?\n",
    "\n",
    "<div><img src=\"files/images/pong.gif\" width=\"30%\" style=\"float: right; margin: 10px;\" align=\"middle\"></div>\n",
    "\n",
    "* El juego: \n",
    "  * controlar una paleta\n",
    "  * +1 pto si la pelota atraviesa la l√≠nea del contrario\n",
    "  * -1 pto si la pelota atraviesa la l√≠nea de la paleta que estamos controlando\n",
    "  * 0 ptos si la pelota no atraviesa ninguna l√≠nea.\n",
    "  \n",
    "* Entrada: Una imagen de 210x160x3\n",
    "* Salida: una acci√≥n (arriba o abajo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Qu√© es Aprendizaje por refuerzo? \n",
    "\n",
    "Es un tipo de aprendizaje que permite a un agente aprender de la **interacci√≥n** con un ambiente por medio de prueba y error, recibiendo como feedback solo un **refuerzo positivo o negativo**.\n",
    "\n",
    "Lo aprendido lo vamos a llamar **pol√≠tica**, la cual es un mapeo de un estado a una acci√≥n.\n",
    "\n",
    "El objetivo es maximizar la recompensa acumulada.\n",
    "\n",
    "![](files/images/rl_notion.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Por qu√©? Cu√°ndo?\n",
    "\n",
    "Cuando necesitamos que un modelo aprenda a **maximizar un objetivo** realizando **acciones en un ambiente**, que pueden tener consecuencias positivas o negativas para el objetivo.\n",
    "\n",
    "\n",
    "Lo aplicamos cuando necesitamos aprender **a partir de los resultados de acciones** que realizamos en el ambiente.\n",
    "\n",
    "- Es posible **simular el ambiente**, pero no se conocen las mejores acciones en cada estado\n",
    "- Son problemas donde se incluye un razonamiento a largo plazo (ej: ajedrez, conducir un auto, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunos ejemplos: Super Mario Bros\n",
    "\n",
    "<img src=\"images/rl_mario.jpeg\" alt=\"Drawing\" style=\"width: 800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunos ejemplos: Doom\n",
    "\n",
    "<img src=\"images/rl_doom.jpg\" alt=\"Drawing\" style=\"width: 800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunos ejemplos: Robots que cocinan\n",
    "\n",
    "<img src=\"images/rl_robot.png\" alt=\"Drawing\" style=\"width: 800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunos ejemplos: Stock market\n",
    "\n",
    "<img src=\"images/rl_stock.png\" alt=\"Drawing\" style=\"width: 800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pol√≠tica\n",
    "\n",
    "La pol√≠tica es lo que **determina qu√© acci√≥n tomar** dado un estado. \n",
    "\n",
    "**Es por lo general, el objetivo del aprendizaje.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay varias formas de modelar un agente que aprende por refuerzos:\n",
    "\n",
    "### Agente basado en utilidad\n",
    "\n",
    "Aprende una **funci√≥n** que le dice qu√© tan **√∫til** es un **estado**. ```f(estado) = utilidad del estado```\n",
    "\n",
    "*Requiere un modelo del ambiente para tomar decisiones*\n",
    "\n",
    "### Agente Q-Learning\n",
    "\n",
    "Aprende una **funci√≥n Q** que le dice qu√© tan **√∫til** es una **acci√≥n dada en un estado dado**. ```f(estado, acci√≥n) = utilidad de tomar esa acci√≥n en ese estado```\n",
    "\n",
    "### Agente reflejo\n",
    "\n",
    "Aprende una funci√≥n que mapea **estados a acciones**. ```f(estado) = acci√≥n a tomar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje Pasivo vs Activo\n",
    "\n",
    "Dependiendo de qu√© nos interesa...\n",
    "\n",
    "- En aprendizaje por refuerzo **pasivo**, buscamos **estimar la utilidad** (qu√© tan buena es) de una pol√≠tica.\n",
    "- En aprendizaje por refuerzo **activo**, buscamos **crear una pol√≠tica** que sea √∫til.\n",
    "\n",
    "En ambos casos, el aprendizaje se hace principalmente a partir de las **recompensas** que el ambiente nos otorga por nuestras **acciones**. Las usamos para actualizar la utilidad estimada de nuestra pol√≠tica, o para modificarla y mejorarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Credit assignment problem\n",
    "\n",
    "Las recompensas se reciben luego de cada acci√≥n tomada, pero...\n",
    "\n",
    "*¬øqu√© sucede cuando la verdadera recompensa demora tiempo en llegar?*\n",
    "\n",
    "*¬øCu√°nta importancia tuvo cada acci√≥n previa para recibir este reward?*\n",
    "\n",
    "<img src=\"images/pong_games.png\" style=\"float: left; margin: 40px; width: 500px\"/>\n",
    "<img src=\"images/checkmate.jpeg\" style=\"height: 300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ¬øEn qu√© momento el agente **actualiza su pol√≠tica** en base a las recompensas recibidas?\n",
    "\n",
    "\n",
    "### M√©todos de Monte Carlo\n",
    "\n",
    "Al final de cada \"episodio\", actualizamos los valores de toda nuestra pol√≠tica entera, en base a la sumatoria de recompensas recibidas en cada acci√≥n. Solo es aplicable cuando tenemos episodios, no cuando es un problema cont√≠nuo. Ej: Pong\n",
    "\n",
    "### M√©todos de aprendizaje por diferencia temporal (TD)\n",
    "\n",
    "Al final de cada \"acci√≥n\", actualizamos los valores relevantes de nuestra pol√≠tica, en base a la recompensa recibida por la acci√≥n tomada. Ej: stocks, Pong\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recompensas en el tiempo\n",
    "\n",
    "*¬øTodas las acciones son igual de importantes?*\n",
    "\n",
    "*¬øLas √∫ltimas son m√°s importantes que las primeras?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Recompensas aditivas\n",
    "\n",
    "La recompensa en un monento dado, es la **suma de todas las recompensas desde ese momento**\n",
    "\n",
    "#### Recompensas depreciativas\n",
    "\n",
    "La recompensa en un momento dado, es la **suma de las recompensas desde ese momento**, pero depreciando las recompensas a medida que son m√°s tard√≠as:\n",
    "\n",
    "$R_{t} = \\sum\\limits_{k=0}^{\\infty} \\gamma^k R_{t + k}$\n",
    "\n",
    "El par√°metro ùõæ (tasa de descuento) var√≠a entre 0 y 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Selecci√≥n de acci√≥n en aprendizaje activo\n",
    "\n",
    "\n",
    "Repasamos los modelos de pol√≠ticas:\n",
    "\n",
    "* Agente basado en utilidad: ```f(estado) = utilidad del estado```\n",
    "* Agente Q-Learning: ```f(estado, acci√≥n) = utilidad de tomar esa acci√≥n en ese estado```\n",
    "* Agente reflejo: ```f(estado) = acci√≥n a tomar```\n",
    "\n",
    "\n",
    "**Dado un modelo, ¬øQu√© acci√≥n debemos tomar?**\n",
    "\n",
    "\n",
    "*Notar la necesidad de un modelo del ambiente en el primer agente*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**¬øQu√© ocurre cuando la pol√≠tica que estamos utilizando a√∫n no fue ajustada?**\n",
    "\n",
    "**¬øQu√© necesitamos hacer para aprender?**\n",
    "\n",
    "**¬øCual es la utilidad de una acci√≥n desconocida?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploraci√≥n vs Explotaci√≥n\n",
    "\n",
    "Necesitamos **explorar** m√°s al inicio (aprender conocimiento), y **explotar** m√°s al final (usar conocimiento aprendidos).\n",
    "\n",
    "Esto lo determina la **funci√≥n de exploraci√≥n**: funci√≥n que dado un estado y acciones disponibles en un momento dado, me determina cu√°l elijo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algunas pol√≠ticas para determinar la acci√≥n\n",
    "\n",
    "* **Greedy:** elige siempre la acci√≥n que maximiza los rewards esperados\n",
    "* **Epsilon greedy:** elige greedy con probabilidad $1-\\epsilon$ y al azar con probabilidad $\\epsilon$\n",
    "* **Decaying epsilon greedy:** epsilon decae a medida que pasa el tiempo\n",
    "\n",
    "<img src=\"images/rewards1.png\" style=\"width: 800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resumiendo hasta ahora:\n",
    "\n",
    "- Idea de pol√≠tica.\n",
    "- Acciones y recibir recompensas en ambiente. Episodios o no.\n",
    "- Armar una pol√≠tica nueva (apr. activo) vs evaluar qu√© tan buena es una pol√≠tica (apr. pasivo).\n",
    "- Pol√≠tica: funci√≥n de valores para los estados, funci√≥n de valores para los pares [estado, acci√≥n a realizar], o tabla estado->acci√≥n.\n",
    "- Actualizar la pol√≠tica al final del episodio (monte carlo) vs al final de cada acci√≥n (TD).\n",
    "- Recompensas aditivas vs depreciativas.\n",
    "- Durante aprendizaje: hacerle o no caso a la pol√≠tica (funci√≥n de exploraci√≥n: exploraci√≥n vs explotaci√≥n dependiendo del tiempo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejemplo: Q-Learning\n",
    "\n",
    "- Para armar una pol√≠tica nueva (apr. activo).\n",
    "- Pol√≠tica: funci√≥n de valores para los pares [estado, acci√≥n a realizar].\n",
    "- Actualizar la pol√≠tica al final de cada acci√≥n (TD).\n",
    "- Recompensas depreciativas.\n",
    "- La funci√≥n de exploraci√≥n no est√° definida (podemos elegir la que queramos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "Q = {}  # Q[state, action] -> Q expected utility for that action in that state. Defaults = 0\n",
    "N = {}  # N[state, action] -> Number of times that action was done from that state. Defaults = 0\n",
    "\n",
    "def q_learning_agent(prev_state, prev_action, state, reward, possible_actions):\n",
    "    # we need to update the knowledge of what happens when we do prev_action in prev_state\n",
    "    # we can only do this *after* that already happened, that's why we need \"prev_\" variables\n",
    "    if prev_state is not None:\n",
    "        # we did the prev_action, count that\n",
    "        N[prev_state, prev_action] += 1\n",
    "        \n",
    "        # how good are the actions we can do now, from where we are??\n",
    "        estimated_rewards_for_possible_actions = [Q[state, action] for action in possible_actions]\n",
    "        best_estimated_next_reward = gamma * max(estimated_rewards_for_possible_actions)\n",
    "        \n",
    "        # update utility estimation of prev_state,prev_action: it's the old value\n",
    "        # (reduced a little bit) plus the new value we know estimate (reduced a lot)\n",
    "        Q[prev_state, prev_action] = (1 - learn_rate) * Q[prev_state, prev_action] + \\\n",
    "                                     learn_rate * (reward + best_estimated_next_reward)          \n",
    "\n",
    "    action = exploration_function(state, possible_actions, Q, N)\n",
    "    return action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generalizaci√≥n\n",
    "\n",
    "*¬øQu√© sucede si queremos aplicar lo visto al Pong usando tablas?*\n",
    "\n",
    "* Esas tablas van a ser muy grandes (probablemente imposible de mantenerlas)\n",
    "* Es imposible pasar por cada posible estado la suficiente cantidad de veces para aprender\n",
    "\n",
    "**La soluci√≥n es usar aproximaci√≥n de funciones** \n",
    "\n",
    "*Encontrar una representaci√≥n del estado que no sea una tabla y una funci√≥n para convertir ese estado en esa representaci√≥n*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Reinforcement Learning\n",
    "\n",
    "Se puede usar una red neuronal como funci√≥n de aproximaci√≥n del estado.\n",
    "\n",
    "<img src=\"images/policy_gradients.png\" style=\"float: right; margin: 40px; width: 500px\"/>\n",
    "\n",
    "\n",
    "Hay varias implementaciones, dos de las m√°s nombradas se basan en ideas antiguas:\n",
    "\n",
    "* Policy gradients\n",
    "* Deep Q-Networks\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
