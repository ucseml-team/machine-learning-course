{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Espacio de discusi贸n\n",
    "\n",
    "#### 驴C贸mo resolver铆an el problema del Pong?\n",
    "\n",
    "<div><img src=\"files/images/pong.gif\" width=\"30%\" style=\"float: right; margin: 10px;\" align=\"middle\"></div>\n",
    "\n",
    "* El juego: \n",
    "  * controlar una paleta\n",
    "  * +1 pto si la pelota atraviesa la l铆nea del contrario\n",
    "  * -1 pto si la pelota atraviesa la l铆nea de la paleta que estamos controlando\n",
    "  * 0 ptos si la pelota no atraviesa ninguna l铆nea.\n",
    "  \n",
    "* Entrada: Una imagen de 210x160x3\n",
    "* Salida: una acci贸n (arriba o abajo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Qu茅 es Aprendizaje por refuerzo? \n",
    "\n",
    "Es un tipo de aprendizaje que permite a un agente aprender de la **interacci贸n** con un ambiente por medio de prueba y error, recibiendo como feedback solo un **refuerzo positivo o negativo**.\n",
    "\n",
    "Lo aprendido lo vamos a llamar **pol铆tica**, la cual es un mapeo de un estado a una acci贸n.\n",
    "\n",
    "El objetivo es maximizar la recompensa acumulada.\n",
    "\n",
    "![](files/images/rl_notion.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Por qu茅? Cu谩ndo?\n",
    "\n",
    "Cuando necesitamos que un modelo aprenda a **maximizar un objetivo** realizando **acciones en un ambiente**, que pueden tener consecuencias positivas o negativas para el objetivo.\n",
    "\n",
    "\n",
    "Lo aplicamos cuando necesitamos aprender **a partir de los resultados de acciones** que realizamos en el ambiente.\n",
    "\n",
    "- Es posible **simular el ambiente**, pero no se conocen las mejores acciones en cada estado\n",
    "- Son problemas donde se incluye un razonamiento a largo plazo (ej: ajedrez, conducir un auto, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunos ejemplos: Super Mario Bros\n",
    "\n",
    "<img src=\"images/rl_mario.jpeg\" alt=\"Drawing\" style=\"width: 800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunos ejemplos: Doom\n",
    "\n",
    "<img src=\"images/rl_doom.jpg\" alt=\"Drawing\" style=\"width: 800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunos ejemplos: Robots que cocinan\n",
    "\n",
    "<img src=\"images/rl_robot.png\" alt=\"Drawing\" style=\"width: 800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Algunos ejemplos: Stock market\n",
    "\n",
    "<img src=\"images/rl_stock.png\" alt=\"Drawing\" style=\"width: 800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pol铆tica\n",
    "\n",
    "La pol铆tica es lo que **determina qu茅 acci贸n tomar** dado un estado. \n",
    "\n",
    "**Es por lo general, el objetivo del aprendizaje.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay varias formas de modelar un agente que aprende por refuerzos:\n",
    "\n",
    "### Agente basado en utilidad\n",
    "\n",
    "Aprende una **funci贸n** que le dice qu茅 tan **煤til** es un **estado**. ```f(estado) = utilidad del estado```\n",
    "\n",
    "*Requiere un modelo del ambiente para tomar decisiones*\n",
    "\n",
    "### Agente Q-Learning\n",
    "\n",
    "Aprende una **funci贸n Q** que le dice qu茅 tan **煤til** es una **acci贸n dada en un estado dado**. ```f(estado, acci贸n) = utilidad de tomar esa acci贸n en ese estado```\n",
    "\n",
    "### Agente reflejo\n",
    "\n",
    "Aprende una funci贸n que mapea **estados a acciones**. ```f(estado) = acci贸n a tomar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje Pasivo vs Activo\n",
    "\n",
    "Dependiendo de qu茅 nos interesa...\n",
    "\n",
    "- En aprendizaje por refuerzo **pasivo**, buscamos **estimar la utilidad** (qu茅 tan buena es) de una pol铆tica.\n",
    "- En aprendizaje por refuerzo **activo**, buscamos **crear una pol铆tica** que sea 煤til.\n",
    "\n",
    "En ambos casos, el aprendizaje se hace principalmente a partir de las **recompensas** que el ambiente nos otorga por nuestras **acciones**. Las usamos para actualizar la utilidad estimada de nuestra pol铆tica, o para modificarla y mejorarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Credit assignment problem\n",
    "\n",
    "Las recompensas se reciben luego de cada acci贸n tomada, pero...\n",
    "\n",
    "*驴qu茅 sucede cuando la verdadera recompensa demora tiempo en llegar?*\n",
    "\n",
    "*驴Cu谩nta importancia tuvo cada acci贸n previa para recibir este reward?*\n",
    "\n",
    "<img src=\"images/pong_games.png\" style=\"float: left; margin: 40px; width: 500px\"/>\n",
    "<img src=\"images/checkmate.jpeg\" style=\"height: 300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 驴En qu茅 momento el agente **actualiza su pol铆tica** en base a las recompensas recibidas?\n",
    "\n",
    "\n",
    "### M茅todos de Monte Carlo\n",
    "\n",
    "Al final de cada \"episodio\", actualizamos los valores de toda nuestra pol铆tica entera, en base a la sumatoria de recompensas recibidas en cada acci贸n. Solo es aplicable cuando tenemos episodios, no cuando es un problema cont铆nuo. Ej: Pong\n",
    "\n",
    "### M茅todos de aprendizaje por diferencia temporal (TD)\n",
    "\n",
    "Al final de cada \"acci贸n\", actualizamos los valores relevantes de nuestra pol铆tica, en base a la recompensa recibida por la acci贸n tomada. Ej: stocks, Pong\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recompensas en el tiempo\n",
    "\n",
    "*驴Todas las acciones son igual de importantes?*\n",
    "\n",
    "*驴Las 煤ltimas son m谩s importantes que las primeras?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Recompensas aditivas\n",
    "\n",
    "La recompensa en un monento dado, es la **suma de todas las recompensas desde ese momento**\n",
    "\n",
    "#### Recompensas depreciativas\n",
    "\n",
    "La recompensa en un momento dado, es la **suma de las recompensas desde ese momento**, pero depreciando las recompensas a medida que son m谩s tard铆as:\n",
    "\n",
    "$R_{t} = \\sum\\limits_{k=0}^{\\infty} \\gamma^k R_{t + k}$\n",
    "\n",
    "El par谩metro  (tasa de descuento) var铆a entre 0 y 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Selecci贸n de acci贸n en aprendizaje activo\n",
    "\n",
    "\n",
    "Repasamos los modelos de pol铆ticas:\n",
    "\n",
    "* Agente basado en utilidad: ```f(estado) = utilidad del estado```\n",
    "* Agente Q-Learning: ```f(estado, acci贸n) = utilidad de tomar esa acci贸n en ese estado```\n",
    "* Agente reflejo: ```f(estado) = acci贸n a tomar```\n",
    "\n",
    "\n",
    "**Dado un modelo, 驴Qu茅 acci贸n debemos tomar?**\n",
    "\n",
    "\n",
    "*Notar la necesidad de un modelo del ambiente en el primer agente*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**驴Qu茅 ocurre cuando la pol铆tica que estamos utilizando a煤n no fue ajustada?**\n",
    "\n",
    "**驴Qu茅 necesitamos hacer para aprender?**\n",
    "\n",
    "**驴Cual es la utilidad de una acci贸n desconocida?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploraci贸n vs Explotaci贸n\n",
    "\n",
    "Necesitamos **explorar** m谩s al inicio (aprender conocimiento), y **explotar** m谩s al final (usar conocimiento aprendidos).\n",
    "\n",
    "Esto lo determina la **funci贸n de exploraci贸n**: funci贸n que dado un estado y acciones disponibles en un momento dado, me determina cu谩l elijo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Algunas pol铆ticas para determinar la acci贸n\n",
    "\n",
    "* **Greedy:** elige siempre la acci贸n que maximiza los rewards esperados\n",
    "* **Epsilon greedy:** elige greedy con probabilidad $1-\\epsilon$ y al azar con probabilidad $\\epsilon$\n",
    "* **Decaying epsilon greedy:** epsilon decae a medida que pasa el tiempo\n",
    "\n",
    "<img src=\"images/rewards1.png\" style=\"width: 800px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resumiendo hasta ahora:\n",
    "\n",
    "- Idea de pol铆tica.\n",
    "- Acciones y recibir recompensas en ambiente. Episodios o no.\n",
    "- Armar una pol铆tica nueva (apr. activo) vs evaluar qu茅 tan buena es una pol铆tica (apr. pasivo).\n",
    "- Pol铆tica: funci贸n de valores para los estados, funci贸n de valores para los pares [estado, acci贸n a realizar], o tabla estado->acci贸n.\n",
    "- Actualizar la pol铆tica al final del episodio (monte carlo) vs al final de cada acci贸n (TD).\n",
    "- Recompensas aditivas vs depreciativas.\n",
    "- Durante aprendizaje: hacerle o no caso a la pol铆tica (funci贸n de exploraci贸n: exploraci贸n vs explotaci贸n dependiendo del tiempo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejemplo: Q-Learning\n",
    "\n",
    "- Para armar una pol铆tica nueva (apr. activo).\n",
    "- Pol铆tica: funci贸n de valores para los pares [estado, acci贸n a realizar].\n",
    "- Actualizar la pol铆tica al final de cada acci贸n (TD).\n",
    "- Recompensas depreciativas.\n",
    "- La funci贸n de exploraci贸n no est谩 definida (podemos elegir la que queramos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "Q = {}  # Q[state, action] -> Q expected utility for that action in that state. Defaults = 0\n",
    "N = {}  # N[state, action] -> Number of times that action was done from that state. Defaults = 0\n",
    "\n",
    "def q_learning_agent(prev_state, prev_action, state, reward, possible_actions):\n",
    "    # we need to update the knowledge of what happens when we do prev_action in prev_state\n",
    "    # we can only do this *after* that already happened, that's why we need \"prev_\" variables\n",
    "    if prev_state is not None:\n",
    "        # we did the prev_action, count that\n",
    "        N[prev_state, prev_action] += 1\n",
    "        \n",
    "        # how good are the actions we can do now, from where we are??\n",
    "        estimated_rewards_for_possible_actions = [Q[state, action] for action in possible_actions]\n",
    "        best_estimated_next_reward = gamma * max(estimated_rewards_for_possible_actions)\n",
    "        \n",
    "        # update utility estimation of prev_state,prev_action: it's the old value\n",
    "        # (reduced a little bit) plus the new value we know estimate (reduced a lot)\n",
    "        Q[prev_state, prev_action] = (1 - learn_rate) * Q[prev_state, prev_action] + \\\n",
    "                                     learn_rate * (reward + best_estimated_next_reward)          \n",
    "\n",
    "    action = exploration_function(state, possible_actions, Q, N)\n",
    "    return action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generalizaci贸n\n",
    "\n",
    "*驴Qu茅 sucede si queremos aplicar lo visto al Pong usando tablas?*\n",
    "\n",
    "* Esas tablas van a ser muy grandes (probablemente imposible de mantenerlas)\n",
    "* Es imposible pasar por cada posible estado la suficiente cantidad de veces para aprender\n",
    "\n",
    "**La soluci贸n es usar aproximaci贸n de funciones** \n",
    "\n",
    "*Encontrar una representaci贸n del estado que no sea una tabla y una funci贸n para convertir ese estado en esa representaci贸n*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Reinforcement Learning\n",
    "\n",
    "Se puede usar una red neuronal como funci贸n de aproximaci贸n del estado.\n",
    "\n",
    "<img src=\"images/policy_gradients.png\" style=\"float: right; margin: 40px; width: 500px\"/>\n",
    "\n",
    "\n",
    "Hay varias implementaciones, dos de las m谩s nombradas se basan en ideas antiguas:\n",
    "\n",
    "* Policy gradients\n",
    "* Deep Q-Networks\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
