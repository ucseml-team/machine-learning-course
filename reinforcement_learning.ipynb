{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Qu√© es? \n",
    "\n",
    "Es un tipo de aprendizaje que permite a un agente aprender de la **interacci√≥n** con un ambiente por medio de prueba y error, recibiendo como feedback solo un **refuerzo positivo o negativo**.\n",
    "\n",
    "Lo aprendido lo vamos a llamar **pol√≠tica**, la cual es un mapeo de un estado a una acci√≥n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](files/images/rl_notion.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Por qu√©? Cu√°ndo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cuando necesitamos que un modelo aprenda a **maximizar un objetivo** realizando **acciones en un ambiente**, que pueden tener consecuencias positivas o negativas para el objetivo.\n",
    "\n",
    "\n",
    "Lo aplicamos cuando necesitamos aprender **a partir de los resultados de acciones** que realizamos en el ambiente.\n",
    "\n",
    "- **No conocemos** del todo el ambiente, necesitamos **explorar** para aprender qu√© cosas dan buenos resultados.\n",
    "- O lo conocemos, pero resulta **impr√°ctico/imposible** analizar **todos** los posibles estados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/rl_mario.jpeg)\n",
    "\n",
    "![](files/images/rl_doom.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/rl_robot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/rl_stock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El aprendizaje se hace **\"en vivo\"** (online), mientras el agente va realizando acciones en un ambiente (real o simulado).\n",
    "\n",
    "Puede que esto suceda en **\"episodios\"** (ej: N partidas de super mario, independientes), o de forma **continua** (ej: acciones en la bolsa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pol√≠tica y resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "La pol√≠tica es lo que me **determina qu√© acci√≥n tomar** en un momento dado. Es por lo general, el objetivo de mi aprendizaje (salvo en pasivo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Hay varias maneras de modelarlo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agente basado en utilidad\n",
    "\n",
    "Aprende una **funci√≥n** que le dice qu√© tan **√∫til** es un **estado**. ```f(estado) = utilidad del estado```\n",
    "\n",
    "\n",
    "### Agente Q-Learning\n",
    "\n",
    "Aprende una **funci√≥n Q** que le dice qu√© tan **√∫til** es una **acci√≥n dada en un estado dado**. ```f(estado, acci√≥n) = utilidad de tomar esa acci√≥n en ese estado```\n",
    "\n",
    "### Agente reflejo\n",
    "\n",
    "Aprende una **\"tabla\"** que mapea **estados a acciones**. ```tabla[estado] = acci√≥n a tomar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nosotros nos vamos a centrar en Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pasivo vs Activo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dependiendo de qu√© nos interesa...\n",
    "\n",
    "- En aprendizaje por refuerzo **pasivo**, buscamos **estimar la utilidad** (qu√© tan buena es) de una pol√≠tica.\n",
    "- En aprendizaje por refuerzo **activo**, buscamos **crear una pol√≠tica** que sea √∫til."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En ambos casos, el aprendizaje se hace principalmente a partir de las **recompensas** que el ambiente nos otorga por nuestras **acciones**. Las usamos para actualizar la utilidad estimada de nuestra pol√≠tica, o para modificarla y mejorarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recompensas en el tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En qu√© momento el agente **recibe** las recompensas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Luego de cada acci√≥n realizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pero en qu√© momento el agente **actualiza su pol√≠tica** en base a las recompensas recibidas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esto depende: podemos actualizarla al final de cada \"episodio\", o podemos ir actualizando despu√©s de cada acci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### M√©todos de Monte Carlo\n",
    "\n",
    "Al final de cada \"episodio\", actualizamos los valores de toda nuestra pol√≠tica entera, en base a la sumatoria de recompensas recibidas en cada acci√≥n. Solo es aplicable cuando tenemos episodios, no cuando es un problema cont√≠nuo.\n",
    "\n",
    "### M√©todos de aprendizaje por diferencia temporal (TD)\n",
    "\n",
    "Al final de cada \"acci√≥n\", actualizamos los valores relevantes de nuestra pol√≠tica, en base a la recompensa recibida por la acci√≥n tomada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Y finalmente, vale lo mismo una recompensa inmediata, que una recompensa demorada?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tenemos dos enfoques:\n",
    "\n",
    "### Recompensas aditivas\n",
    "\n",
    "La recompensa en un monento dado, es la **suma de todas las recompensas hasta ese momento**:\n",
    "\n",
    "```R = Rt1 + Rt2 + Rt3 + ...```\n",
    "\n",
    "### Recompensas depreciativas\n",
    "\n",
    "La recompensa en un monento dado, es la **suma de las recompensas hasta ese momento**, pero depreciando las recompensas a medida que son m√°s tard√≠as:\n",
    "\n",
    "```R = ùõæRt1 + ùõæ¬≤Rt2 + ùõæ¬≥Rt3 + ...```\n",
    "\n",
    "El par√°metro ùõæ var√≠a entre 0 y 1. Cuando es 1, las recompensas no pierden valor. Cuanto m√°s cercano a 1, menos valor pierden. Y viceversa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Selecci√≥n de acci√≥n en aprendizaje activo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sabemos qu√© vamos a hacer con la recompensa despu√©s de que tomemos la acci√≥n (la usamos para evaluar y/o mejorar nuestra pol√≠tica). Pero qu√© acci√≥n tomamos??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Siempre la que creemos **mejor** en base a nuestra pol√≠tica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pan duro en frente del agente, vs torta en otra habitaci√≥n. Nunca va a conocer las mejores recompensas, se va a quedar solo con las primeras que encuentre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cualquiera? Al **azar**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vamos a estar a√±os y a√±os realizando acciones al azar. En la mayor√≠a de los problemas, son m√°s las acciones in√∫tiles o malas, que las que dan buenos resultados.\n",
    "\n",
    "Vamos a demorar eternidades en encontrar una buena pol√≠tica!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploraci√≥n vs Explotaci√≥n\n",
    "\n",
    "Necesitamos **explorar** m√°s al inicio (aprender conocimiento), y **explotar** m√°s al final (usar conocimiento aprendidos).\n",
    "\n",
    "Esto lo determina la **funci√≥n de exploraci√≥n**: funci√≥n que dado un estado y acciones disponibles en un momento dado, me determina cu√°l elijo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ejemplo simple y malo: \n",
    "```\n",
    "if t < X: \n",
    "    al azar\n",
    "else:\n",
    "    la mejor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Normalmente buscamos funciones con transici√≥n m√°s suave.\n",
    "\n",
    "Inteligentes como para al inicio dar **m√°s chances** a acciones **no realizadas** en estados **no vistos**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Selecci√≥n de acci√≥n vs modelado de la pol√≠tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Repasamos los modelos de pol√≠ticas:\n",
    "\n",
    "* Agente basado en utilidad: ```f(estado) = utilidad del estado```\n",
    "* Agente Q-Learning: ```f(estado, acci√≥n) = utilidad de tomar esa acci√≥n en ese estado```\n",
    "* Agente reflejo: ```tabla[estado] = acci√≥n a tomar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Qu√© pasa cuando el agente basado en utilidad recibe un estado y debe tomar una acci√≥n? C√≥mo elije qu√© acci√≥n tomar de las disponibles?\n",
    "\n",
    "Necesita poder **predecir** el estado al que va a llegar con cada acci√≥n, para ver cu√°l es el mejor, y elegir la acci√≥n correspondiente.\n",
    "\n",
    "Esto **no pasa** con los otros dos agentes, que solo elijen una acci√≥n, sin necesariamente conocer a qu√© estado lleva.\n",
    "\n",
    "El agente basado en utilidad necesita **un modelo mental del ambiente**. No siempre tenemos eso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resumiendo hasta ahora:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Idea de pol√≠tica.\n",
    "- Acciones y recibir recompensas en ambiente. Episodios o no.\n",
    "- Armar una pol√≠tica nueva (apr. activo) vs evaluar qu√© tan buena es una pol√≠tica (apr. pasivo).\n",
    "- Pol√≠tica: funci√≥n de valores para los estados, funci√≥n de valores para los pares [estado, acci√≥n a realizar], o tabla estado->acci√≥n.\n",
    "- Actualizar la pol√≠tica al final del episodio (monte carlo) vs al final de cada acci√≥n (TD).\n",
    "- Recompensas aditivas vs depreciativas.\n",
    "- Durante aprendizaje: hacerle o no caso a la pol√≠tica (funci√≥n de exploraci√≥n: exploraci√≥n vs explotaci√≥n dependiendo del tiempo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejemplo: Q-Learning\n",
    "\n",
    "- Mejor con episodios.\n",
    "- Para armar una pol√≠tica nueva (apr. activo).\n",
    "- Pol√≠tica: funci√≥n de valores para los pares [estado, acci√≥n a realizar].\n",
    "- Actualizar la pol√≠tica al final de cada acci√≥n (TD).\n",
    "- Recompensas depreciativas.\n",
    "- La funci√≥n de exploraci√≥n no est√° definida (podemos elegir la que queramos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "Q = {}  # Q[state, action] -> Q expected utility for that action in that state. Defaults = 0\n",
    "N = {}  # N[state, action] -> Number of times that action was done from that state. Defaults = 0\n",
    "\n",
    "def q_learning_agent(prev_state, prev_action, state, reward, possible_actions):\n",
    "    # we need to update the knowledge of what happens when we do prev_action in prev_state\n",
    "    # we can only do this *after* that already happened, that's why we need \"prev_\" variables\n",
    "    if prev_state is not None:\n",
    "        # we did the prev_action, count that\n",
    "        N[prev_state, prev_action] += 1\n",
    "        \n",
    "        # how good are the actions we can do now, from where we are??\n",
    "        estimated_rewards_for_possible_actions = [Q[state, action] for action in possible_actions]\n",
    "        best_estimated_next_reward = gamma * max(estimated_rewards_for_possible_actions)\n",
    "        \n",
    "        # update utility estimation of prev_state,prev_action: it's the old value\n",
    "        # (reduced a little bit) plus the new value we know estimate (reduced a lot)\n",
    "        Q[prev_state, prev_action] = (1 - learn_rate) * Q[prev_state, prev_action] + \\\n",
    "                                     learn_rate * (reward + best_estimated_next_reward)          \n",
    "\n",
    "    action = exploration_function(state, possible_actions, Q, N)\n",
    "    return action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Y redes neuronales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pueden las redes neuronales aportar algo a aprendizaje por refuerzo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Deep Reinforcement Learning\n",
    "\n",
    "Muy **similar a Q-Learning**. \n",
    "\n",
    "En lugar de armar a mano la **funci√≥n Q(estado, acci√≥n)**, lo que hacemos es que **una red neuronal sea esa funci√≥n**, y que se **entrene a partir de las recompensas** que va recibiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/qlearning_vs_drl.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**La red neuronal es la pol√≠tica.**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
