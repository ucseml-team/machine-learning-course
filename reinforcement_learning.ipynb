{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Qu茅 es? \n",
    "\n",
    "Es un tipo de aprendizaje que permite a un agente aprender de la **interacci贸n** con un ambiente por medio de prueba y error, recibiendo como feedback solo un **refuerzo positivo o negativo**.\n",
    "\n",
    "Lo aprendido lo vamos a llamar **pol铆tica**, la cual es un mapeo de un estado a una acci贸n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/rl_notion.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Por qu茅? Cu谩ndo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cuando necesitamos que un modelo aprenda a **maximizar un objetivo** realizando **acciones en un ambiente**, que pueden tener consecuencias positivas o negativas para el objetivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/rl_mario.jpeg)\n",
    "\n",
    "![](files/images/rl_doom.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/rl_robot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/rl_stock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lo aplicamos cuando necesitamos aprender **a partir de los resultados de acciones** que realizamos en el ambiente.\n",
    "\n",
    "- **No conocemos** del todo el ambiente, necesitamos **explorar** para aprender qu茅 cosas dan buenos resultados.\n",
    "- O lo conocemos, pero resulta **impr谩ctico/imposible** analizar **todos** los posibles estados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "El aprendizaje se hace **\"en vivo\"** (online), mientras el agente va realizando acciones en un ambiente (real o simulado).\n",
    "\n",
    "Puede que esto suceda en **\"episodios\"** (ej: N partidas de super mario, independientes), o de forma **continua** (ej: acciones en la bolsa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/ml_process.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pol铆tica y resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "La pol铆tica es lo que me **determina qu茅 acci贸n tomar** en un momento dado. Es por lo general, el objetivo de mi aprendizaje (salvo en pasivo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hay varias maneras de modelarlo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Agente basado en utilidad\n",
    "\n",
    "Aprende una **funci贸n** que le dice qu茅 tan **煤til** es un **estado**. ```f(estado) = utilidad del estado```\n",
    "\n",
    "\n",
    "### Agente Q-Learning\n",
    "\n",
    "Aprende una **funci贸n Q** que le dice qu茅 tan **煤til** es una **acci贸n dada en un estado dado**. ```f(estado, acci贸n) = utilidad de tomar esa acci贸n en ese estado```\n",
    "\n",
    "### Agente reflejo\n",
    "\n",
    "Aprende una **\"tabla\"** que mapea **estados a acciones**. ```tabla[estado] = acci贸n a tomar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nosotros nos vamos a centrar en Q-Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pasivo vs Activo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dependiendo de qu茅 nos interesa...\n",
    "\n",
    "- En aprendizaje por refurezo **pasivo**, buscamos **estimar la utilidad** (qu茅 tan buena es) de una pol铆tica.\n",
    "- En aprendizaje por refuerzo **activo**, buscamos **crear una pol铆tica** que sea 煤til."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En ambos casos, el aprendizaje se hace principalmente a partir de las **recompensas** que el ambiente nos otorga por nuestras **acciones**. Las usamos para actualizar la utilidad estimada de nuestra pol铆tica, o para modificarla y mejorarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recompensas en el tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "En qu茅 momento el agente **recibe** las recompensas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Luego de cada acci贸n realizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pero en qu茅 momento el agente **actualiza su pol铆tica** en base a las recompensas recibidas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Esto depende: podemos actualizarla al final de cada \"episodio\", o podemos ir actualizando despu茅s de cada acci贸n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### M茅todos de Monte Carlo\n",
    "\n",
    "Al final de cada \"episodio\", actualizamos los valores de toda nuestra pol铆tica entera, en base a la sumatoria de recompensas recibidas en cada acci贸n. Solo es aplicable cuando tenemos episodios, no cuando es un problema cont铆nuo.\n",
    "\n",
    "### M茅todos de aprendizaje por diferencia temporal (TD)\n",
    "\n",
    "Al final de cada \"acci贸n\", actualizamos los valores relevantes de nuestra pol铆tica, en base a la recompensa recibida en la acci贸n.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Y finalmente, vale lo mismo una recompensa inmediata, que una recompensa demorada?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tenemos dos enfoques:\n",
    "\n",
    "### Recompensas aditivas\n",
    "\n",
    "La recompensa en un monento dado, es la **suma de todas las recompensas hasta ese momento**:\n",
    "\n",
    "```R = Rt1 + Rt2 + Rt3 + ...```\n",
    "\n",
    "### Recompensas depreciativas\n",
    "\n",
    "La recompensa en un monento dado, es la **suma de las recompensas hasta ese momento**, pero depreciando las recompensas a medida que son m谩s tard铆as:\n",
    "\n",
    "```R = Rt1 + 韭Rt2 + 韭Rt3 + ...```\n",
    "\n",
    "El par谩metrro  var铆a entre 0 y 1. Cuando es 1, las recompensas no pierden valor. Cuanto m谩s cercano a 1, menos valor pierden. Y viceversa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Selecci贸n de acci贸n en aprendizaje activo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sabemos qu茅 vamos a hacer con la recompensa despu茅s de que tomemos la acci贸n (la usamos para evaluar y/o mejorar nuestra pol铆tica). Pero qu茅 acci贸n tomamos??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Siempre la que creemos **mejor** en base a nuestra pol铆tica?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pan duro en frente del agente, vs torta en otra habitaci贸n. Nunca va a conocer las mejores recompensas, se va a quedar solo con las primeras que encuentre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Cualquiera? Al **azar**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vamos a estar a帽os y a帽os realizando acciones al azar. En la mayor铆a de los problemas, son m谩s las acciones in煤tiles o malas, que las que dan buenos resultados.\n",
    "\n",
    "Vamos a demorar eternidades en encontrar una buena pol铆tica!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exploraci贸n vs Explotaci贸n\n",
    "\n",
    "Necesitamos **explorar** m谩s al inicio (aprender conocimiento), y **explotar** m谩s al final (usar conocimiento aprendidos).\n",
    "\n",
    "Esto lo determina la **funci贸n de exploraci贸n**: funci贸n que dado un estado y acciones disponibles en un momento dado, me determina cu谩l elijo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ejemplo simple y malo: \n",
    "```\n",
    "if t < X: \n",
    "    al azar\n",
    "else:\n",
    "    la mejor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Normalmente buscamos funciones con transici贸n m谩s suave.\n",
    "\n",
    "Inteligentes como para al inicio dar **m谩s chances** a acciones **no realizadas** en estados **no vistos**, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Selecci贸n de acci贸n vs modelado de la pol铆tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Repasamos los modelos de pol铆ticas:\n",
    "\n",
    "* Agente basado en utilidad: ```f(estado) = utilidad del estado```\n",
    "* Agente Q-Learning: ```f(estado, acci贸n) = utilidad de tomar esa acci贸n en ese estado```\n",
    "* Agente reflejo: ```tabla[estado] = acci贸n a tomar```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Qu茅 pasa cuando el agente basado en utilidad recibe un estado y debe tomar una acci贸n? C贸mo elije qu茅 acci贸n tomar de las disponibles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Necesita poder **predecir** el estado al que va a llegar con cada acci贸n, para ver cu谩l es el mejor, y elegir la acci贸n correspondiente.\n",
    "\n",
    "Esto **no pasa** con los otros dos agentes, que solo elijen una acci贸n, sin necesariamente conocer a qu茅 estado lleva.\n",
    "\n",
    "El agente basado en utilidad necesita **un modelo mental del ambiente**. No siempre tenemos eso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Resumiendo hasta ahora:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Idea de pol铆tica.\n",
    "- Acciones y recibir recompensas en ambiente. Episodios o no.\n",
    "- Armar una pol铆tica nueva (apr. activo) vs o evaluar qu茅 tan buena es una pol铆tica (apr. pasivo).\n",
    "- Pol铆tica: funci贸n de valores para los estados, funci贸n de valores para los pares [estado, acci贸 a realizar], o tabla estado->acci贸n.\n",
    "- Actualizar la pol铆tica al final del episodio (monte carlo) vs al final de cada acci贸n (TD).\n",
    "- Recompensas aditivas vs depreciativas.\n",
    "- Durante aprendizaje: hacerle o no caso a la pol铆tica (funci贸n de exploraci贸n: exploraci贸n vs explotaci贸n dependiendo del tiempo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Ejemplo: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Mejor con episodios.\n",
    "- Para armar una pol铆tica nueva (apr. activo).\n",
    "- Pol铆tica: funci贸n de valores para los pares [estado, acci贸 a realizar].\n",
    "- Actualizar la pol铆tica al final de cada acci贸n (TD).\n",
    "- Recompensas depreciativas.\n",
    "- La funci贸n de exploraci贸n no est谩 definida (podemos elegir la que queramos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "Q = {}  # Q[state, action] -> Q expected utility for that action in that state. Defaults = 0\n",
    "N = {}  # N[state, action] -> Number of times that action was done from that state. Defaults = 0\n",
    "\n",
    "def q_learning_agent(prev_state, prev_action, state, reward, possible_actions):\n",
    "    # we need to update the knowledge of what happens when we do prev_action in prev_state\n",
    "    # we can only do this *after* that already happened, that's why we need \"prev_\" variables\n",
    "    if prev_state is not None:\n",
    "        # we did the prev_action, count that\n",
    "        N[prev_state, prev_action] += 1\n",
    "        \n",
    "        # how good are the actions we can do now, from where we are??\n",
    "        estimated_rewards_for_possible_actions = [Q[state, action] for action in possible_actions]\n",
    "        best_estimated_next_reward = gamma * max(estimated_rewards_for_possible_actions)\n",
    "        \n",
    "        # update utility estimation of prev_state,prev_action: it's the old value\n",
    "        # (reduced a little bit) plus the new value we know estimate (reduced a lot)\n",
    "        Q[prev_state, prev_action] = (1 - learn_rate) * Q[prev_state, prev_action] + \\\n",
    "                                     learn_rate * (reward + best_estimated_next_reward)          \n",
    "\n",
    "    action = exploration_function(state, possible_actions, Q, N)\n",
    "    return action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Y redes neuronales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pueden las redes neuronales aportar algo a aprendizaje por refuerzo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Deep Reinforcement Learning\n",
    "\n",
    "Muy **similar a Q-Learning**. \n",
    "\n",
    "En lugar de armar a mano la **funci贸n Q(estado, acci贸n)**, lo que hacemos es que **una red neuronal sea esa funci贸n**, y que se **entrene a partir de las recompensas** que va recibiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](files/images/qlearning_vs_drl.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**La red neuronal es la pol铆tica.**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
